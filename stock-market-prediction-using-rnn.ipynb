{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas_datareader\n!pip install yfinance","metadata":{"scrolled":true,"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport yfinance as yf\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, GRU\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.random import set_seed\n\nfrom pandas_datareader.data import DataReader\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nset_seed(455)\nnp.random.seed(455)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download Apple's stock market prices.","metadata":{}},{"cell_type":"code","source":"end = datetime.now()\nstart = datetime(2016, end.month, end.day)\ndataset = yf.download(\"AAPL\", start, end)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tstart = 2016\ntend = 2020\n\ndef train_test_plot(dataset, tstart, tend):\n    dataset.loc[f\"{tstart}\":f\"{tend}\", \"High\"].plot(figsize=(16, 4), legend=True)\n    dataset.loc[f\"{tend+1}\":, \"High\"].plot(figsize=(16, 4), legend=True)\n    plt.legend([f\"Train (Before {tend+1})\", f\"Test ({tend+1} and beyond)\"])\n    plt.title(\"APPLE stock price\")\n    plt.show()\n\ntrain_test_plot(dataset,tstart,tend)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split(dataset, tstart, tend):\n    train = dataset.loc[f\"{tstart}\":f\"{tend}\", \"High\"]\n    test = dataset.loc[f\"{tend+1}\":, \"High\"]\n    return train, test\n\ndef train_test_split_values(dataset, tstart, tend):\n    train, test =  train_test_split(dataset, tstart, tend)\n    return train.values, test.values\n\ntraining_set, test_set = train_test_split_values(dataset, tstart, tend)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling the training set\nsc = MinMaxScaler(feature_range=(0, 1))\ntraining_set = training_set.reshape(-1, 1)\ntraining_set_scaled = sc.fit_transform(training_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_sequence(sequence, window):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        end_ix = i + window\n        if end_ix > len(sequence) - 1:\n            break\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)\n\nwindow_size = 60\nfeatures = 1\n\nX_train, y_train = split_sequence(training_set_scaled, window_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN","metadata":{}},{"cell_type":"markdown","source":"RNN stands for Recurrent Neural Network, a type of artificial neural network designed to work with sequences of data, such as time series or natural language. Unlike feedforward neural networks, which process input data in a single pass and generate output, RNNs maintain a hidden state that can capture information from previous inputs in the sequence. This allows them to exhibit temporal dynamic behavior and learn patterns or dependencies over time.\n\nRNNs consist of interconnected layers of neurons, with recurrent connections that allow information to loop back within the network. This recurrent nature enables the network to \"remember\" previous inputs and use that information in processing subsequent inputs. However, RNNs can suffer from issues like vanishing or exploding gradients, which make it difficult for them to learn long-range dependencies.\n\nTo address these limitations, more advanced variants of RNNs, such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), have been developed. These architectures introduce specialized gating mechanisms that help to control the flow of information, making it easier to capture and learn from long-range dependencies in the data.","metadata":{}},{"cell_type":"markdown","source":"# LTSM","metadata":{}},{"cell_type":"code","source":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(units=125, activation=\"tanh\", input_shape=(window_size, features)))\nmodel_lstm.add(Dense(25))\nmodel_lstm.add(Dense(units=1))\n\nmodel_lstm.compile(optimizer='adam', loss='mse')\n\nmodel_lstm.summary()","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lstm.fit(X_train, y_train, epochs=15, batch_size=32)","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_total = dataset.loc[:,\"High\"]\ninputs = dataset_total[len(dataset_total) - len(test_set) - window_size :].values\ninputs = inputs.reshape(-1, 1)\ninputs = sc.transform(inputs)","metadata":{"_kg_hide-input":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test, y_test = split_sequence(inputs, window_size)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], features)\npredicted_stock_price = model_lstm.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\ny_test = sc.inverse_transform(y_test)","metadata":{"_kg_hide-output":false,"_kg_hide-input":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_predictions(test, predicted):\n    plt.plot(test, color=\"gray\", label=\"Real\")\n    plt.plot(predicted, color=\"red\", label=\"Predicted\")\n    plt.title(\"Stock Price Prediction\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Stock Price\")\n    plt.legend()\n    plt.show()\n\n\ndef return_rmse(test, predicted):\n    rmse = np.sqrt(mean_squared_error(test, predicted))\n    print(\"The root mean squared error is {:.2f}.\".format(rmse))","metadata":{"_kg_hide-input":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_predictions(y_test,predicted_stock_price)","metadata":{"_kg_hide-input":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return_rmse(y_test,predicted_stock_price)","metadata":{"_kg_hide-input":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GRU","metadata":{}},{"cell_type":"code","source":"model_gru = Sequential()\nmodel_gru.add(GRU(units=125, activation=\"tanh\", input_shape=(window_size, features)))\nmodel_gru.add(Dense(units=1))\n\nmodel_gru.compile(optimizer=\"adam\", loss=\"mse\")\n\nmodel_gru.summary()","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_gru.fit(X_train, y_train, epochs=10, batch_size=32)","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GRU_predicted_stock_price = model_gru.predict(X_test)\nGRU_predicted_stock_price = sc.inverse_transform(GRU_predicted_stock_price)","metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_predictions(y_test, GRU_predicted_stock_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return_rmse(y_test,GRU_predicted_stock_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: GRU lead to better results.","metadata":{}}]}